{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AUC Coding Challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/schwaaweb/aimlds1_07-TheMachineLearningFramework/blob/master/T07_CC--DJ--AUC_Coding_Challenge.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "aD8Yb0P8D04e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Area-Under-Curve Measurement\n",
        "\n",
        "Objectives:\n",
        "* Compute true positive, true negative, false positive, and false negative classification rate\n",
        "* Adjust classification threshold to control above rates\n",
        "* Plot AUC  to demonstrate accuracy\n",
        "* Calculate AUC as a single value\n",
        "\n",
        "### Background\n",
        "\n",
        "You want to be familiar with ROC and AUC.\n",
        "\n",
        "#### Receiver Operating Characteristic Curve (ROC)\n",
        "\n",
        "[wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
        "[Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall) [sklearn](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)\n",
        "\n",
        "#### Area Under Curve (AUC)\n",
        "\n",
        "[sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html)\n",
        "\n",
        "The ROC curve provides a visualization of the performance of a classifier, with false-positive rates along the $x$ axis and true-positive rates along the $y$ axis. A perfect classifier produces no false positives (100% specificity) and no false negatives (100% sensitivity). In reality, classifiers are not so discerning.\n",
        "\n",
        "### Do:\n",
        "\n",
        "Correct the bugs in the following code blocks in order to work through the mechanics of building an AUC. \n",
        "\n",
        "Once you've calculated `FPR` and `TPR` for the Logistic Regression classifier on its Sprint Challenge dataset and visualized the results, modify it to compute the same data using the _test_ data, instead of the _train_ data."
      ]
    },
    {
      "metadata": {
        "id": "9JrCk2HEPwoI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Compute Linear Regression Model\n",
        "\n",
        "Create a training set and train a Logistic Regression model with it"
      ]
    },
    {
      "metadata": {
        "id": "5UeD3N5_PwCF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('https://www.dropbox.com/s/bnwfu81bjpf22hp/logistic_regression.csv?raw=1')\n",
        "# Use train_test_split to create a training set\n",
        "train_x, test_x, train_y, test_y = train_test_split(data[['x1','x2']],data['y'])\n",
        "\n",
        "# Create and train(fit) the model\n",
        "regr = LogisticRegression()\n",
        "regr.fit(train_x, train_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSfpHHfJFPc5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Predict values for $\\hat{y}$ for the test set"
      ]
    },
    {
      "metadata": {
        "id": "x3K306_pFUQc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Predict y_hat for the test set created previously\n",
        "\n",
        "y_hat = regr.predict(train_x)\n",
        "print(y_hat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xS6VlmpYTzPx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Observe precision"
      ]
    },
    {
      "metadata": {
        "id": "Fv_8vZvrT44v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# True positive: if the prediction is 1, and the original data is 1, their sum is 2\n",
        "true_positives = (y_hat+train_y)==2\n",
        "# False positive: if the prediction is 1, but the original data is 0, their sum is 1\n",
        "false_positives = (y_hat-train_y)=1\n",
        "print(true_positives.T)\n",
        "print(false_positives.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yybc_0TTTo6s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The probabilities computed by Logistic Regression:\n",
        "# The columns correspond with class 0 or 1\n",
        "proba = regr.predict_proba(train_x)\n",
        "print(proba)\n",
        "\n",
        "# The decision function computed by Logistic Regression - this is simply\n",
        "# \\sum w_i x_i + b, before the logit function has been applied\n",
        "dec = regr.decision_function(train_x)\n",
        "print(dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PYmuA8VwJ4_y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Build ROC curves"
      ]
    },
    {
      "metadata": {
        "id": "Jcyi6Z16OGuM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "thresholds = np.linspace(0.0,1.0,11)\n",
        "predictions = pd.DataFrame()\n",
        "\n",
        "for decision_threshold in thresholds:\n",
        "    decision_threshold = np.round(decision_threshold,2)\n",
        "    true_false_array = (proba[::,1:2:] > decision_threshold).T[0]\n",
        "    predictions[str(decision_threshold] = true_false_array.astype(int)\n",
        "        \n",
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDEdBrtYFjEr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Calculate TP and FP at each discrimination level"
      ]
    },
    {
      "metadata": {
        "id": "B73O-XcrF9Wd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Base case:\n",
        "false_positives = np.sum(predictions['0.1']-train_y.values>1)\n",
        "true_positives = np.sum(predictions['0.1']+train_y.values==2)\n",
        "print('False positive rate at threshold 0.1: ',false_positives/np.sum(train_y==0))\n",
        "print('True positive rate at threshold 0.1: ',true_positives/np.sum(train_y==1))\n",
        "\n",
        "# Now automate the base case across the set of all decision thresholds\n",
        "fp = pd.DataFrame()\n",
        "tp = pd.DataFrame()\n",
        "for threshold in predictions.columns:\n",
        "    fp[threshold] = [npsum(predictions[threshold]-train_y.values==1)/np.sum(train_y==0)]\n",
        "    tp[threshold] = [np.sum(predictions[threshold]+train_y.values==2)/np.sum(train_y==1)]\n",
        "\n",
        "# The false positives and true positive rated, based on the sliding decision threshold\n",
        "print(fp.T)\n",
        "print(tp.T)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(fp.T,tp.T);\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jhg7EM4YQ6nG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Example using sklearn"
      ]
    },
    {
      "metadata": {
        "id": "t2xGVuYcQfAo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O1_hD-raG2s2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The Area-Under-The-Curve Score\n",
        "print(metrics.auc(fp.T,tp.T))\n",
        "print(metrics.roc_auc_score(train_y.values,proba[::,1:2:]))\n",
        "\n",
        "# Producing the same false/true positive data via a library and plotting it\n",
        "fpr, tpr, _ = metrics.roc_curve(train_y.values,proba[::,1:2:])\n",
        "print(fpr)\n",
        "print(tpr)\n",
        "plt.plot(fpr,tpr);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QS0KgX-_QbX6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Do\n",
        "\n",
        "### 5. Compute the ROC and AUC using the _testset_ instead of the _trainset_\n"
      ]
    },
    {
      "metadata": {
        "id": "CVtzXgERHI4B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compute and plot"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}