{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Tuning Assignment.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/schwaaweb/aimlds1_07-TheMachineLearningFramework/blob/master/M07_A--Model_Tuning_Assignment.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "j3-9qZ80euzF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part One: K-fold Cross Validation\n",
        "\n",
        "The challenge of training machine learning models is to be able to make accurate predictions on previously unseen real-world data in spite of the fact that we only have a finite training dataset to learn from. \n",
        "\n",
        "One way of validating our model's quality-of-fit and avoiding overfitting/underfitting, is to use the test_train_split method like we did in the code challenge. With this method, the randomly selected test dataset can be used to evaluate how our model performs on data that it has not yet seen in the training process. However, there are downsides to this approach:\n",
        "\n",
        "*   We lose a valuable portion of data that we would prefer to be able to train on to serve as the test dataset. We would prefer to have both the testing and training datasets be as large as possible.\n",
        "*   With small datasets, measures of our model's quality using the test_train_split method often have a high variance. (We saw this behavior when we changed the random seed in the code challenge)\n",
        "\n",
        "We can reduce the severity of both of these drawbacks by using what is called K-fold Cross Validation:\n",
        "\n",
        "[Short Video Explaining K-Fold Cross Validation](https://www.youtube.com/watch?v=TIgfjmp-4BA)\n",
        "\n",
        "[How to Implement K-Fold Cross Validation on the Pima Indians Diabetes dataset](https://machinelearningmastery.com/evaluate-performance-machine-learning-algorithms-python-using-resampling/)"
      ]
    },
    {
      "metadata": {
        "id": "8-xSRguyK0SS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## DO THIS:\n",
        "\n",
        "**1)** Train a logistic regression model on the titanic dataset predicting survivors first using a 20-80% test_train_split and print the accuracy of your model using 5 different random seeds.\n",
        "\n",
        "**2)** Use 5-fold Cross Validation on the titanic dataset. Print out the accuracies from each of the 5 folds of the cross validation, then print the final mean and standard deviation of those cross validation accuracies. How do the accuracies on each of the inidvidual folds compare to the accuracies of the test_train_split approach? Is the variance in accuracies of the cross-validation approach higher or lower than the variance of the test_train_split approach? \n",
        "\n",
        "**3)** Try using 3-fold Cross Validation as well as 10-fold cross validation. How does the number of folds in the cross-validation process affect the outcome? How many folds should be used?\n",
        "\n",
        "---\n",
        "I would give you more boilerplate code here, but I don't want to make it too easy. The articles linked above should be sufficient for this purpose."
      ]
    },
    {
      "metadata": {
        "id": "kqwcuyA9K6dq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### YOUR CODE HERE ##### - Feel free to add code cells as necessary."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1qRSmAPZo0Qr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part Two: Hyperparameter Tuning\n",
        "\n",
        "An important technique for improving the accuracy of a machine learning model is to undertake a process known as Hyperparameter Tuning or Hyperparameter Optimization. In order to understand this process, we first need to understand the difference between a model parameter and a model hyperparameter. \n",
        "\n",
        "### What is a model parameter?\n",
        "\n",
        "A model parameter is a value that is generated by fitting our model to training data and is key to generating predictions with that model. They are **internal** to our model and we often are trying to estimate them as best as possible when we train the algorithm. \n",
        "\n",
        "For example, the parameters of a linear regression model would be its intercept value as well as the coefficient values on each of the X variables. Estimates of these crucial values (parameters) are obtained by fitting to the training data, perfectly define the model, are internal to the model, and are key to generating predictions. They are model parameters in every sense. \n",
        "\n",
        "### What is a model hyperparameter?\n",
        "\n",
        "Hyperparameters are values that are key to how well our algorithm runs, yet are **external** to our model and cannot be estimated from the training process. They are more like settings for our algorithm which must be designated before it is run and impact its performance. Here is some further reading:\n",
        "\n",
        "[Hyperparamters explanation on Quora](https://www.quora.com/What-are-hyperparameters-in-machine-learning)\n",
        "\n",
        "[Jason Brownlee Article on the difference between Parameters and Hyperparameters](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)\n",
        "\n",
        "### How do we find the best hyperparameters?\n",
        "\n",
        "Since we can't learn the best hyperparameters for our model from the data, we essentially just pick values and see which ones lead to the highest accuracy. This can be a tedious and complex process especially for certain models like neural networks which can have dozens of hyperparameters. We will get you familiar with the process using a more simple logistic regression model. \n",
        "\n",
        "### How do you know what hyperparameters exist for your particular model? \n",
        "\n",
        "Most models/libraries have default hyperparameters that will be used if we don't specify them. In the model selection process you might try out multiple models on a dataset and see which one gets you the highest out-of-the-box performance, (using the default hyperparameters) and then pick a couple of the highest performing algorithms and attempt hyperparameter tuning on them to compare how different models benefit from this process. Once you have narrowed down the models that you would like to tune, a quick google search can tell you what hyperparameters exist for that algorithm. \n",
        "\n",
        "Often you can learn about potential hyperparameters by looking at the documentation for a given algorithm in a library, here's the documentation for sklearn's logistic regression, see if you can spot the hyperparameters:\n",
        "\n",
        "[scikit-learn logistic regression docs](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "wADqKJAgSqgI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## DO THIS: \n",
        "\n",
        "Lets hyperparameter tune our **titanic** predictions using 5-fold cross validation to compare the accuracy of our tuned models. \n",
        "\n",
        "### Manual Hyperparameter Tuning:\n",
        "\n",
        "For our assignment today we are going to tune the 'C value' also known as the 'regularization strength' of our logistic regression as well as 'penalty' of our logistic regression algorithm.\n",
        "\n",
        "Read up on the regularlization strength and penalty of a logistic regression function. What might be some good values to test out? Hint: Look at the parameter definitions on the sci-kit learn logistic regression documentation. \n",
        "\n",
        "[scikit-learn logistic regression docs](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html)\n",
        "\n",
        "[Regularization in Logistic Regression](https://www.kdnuggets.com/2016/06/regularization-logistic-regression.html)\n",
        "\n",
        "Fit your model 5 different times using 5 different C values of your choosing. Which value gives the highest accuracy? \n",
        "\n",
        "There are only two penalty values that we can use. Evaluate the model two more times using each penalty once. Which penalty gives the highest accuracy?"
      ]
    },
    {
      "metadata": {
        "id": "UJujhJnetnr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1269a01-5911-4228-8aae-db83529a4dfc"
      },
      "cell_type": "code",
      "source": [
        "# The sample code below uses the Pima Indans Diabetes Dataset. \n",
        "# Here we are setting the C value hyperparameter to 1 and the penalty hyperparameter to \"l1\". \n",
        "# You can designate your hyperparameters in a similar fashion.\n",
        "\n",
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = pandas.read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "num_instances = len(X)\n",
        "seed = 7\n",
        "kfold = model_selection.KFold(n_splits=5, random_state=seed)\n",
        "model = LogisticRegression(C=1, penalty='l1') ##### This is the important line\n",
        "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.75974026 0.72727273 0.76623377 0.83006536 0.76470588]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aq8mXV2rbWCS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### YOUR CODE HERE ##### - Feel free to add as many code cells as necessary."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CrwljGvubkpv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1) Grid-Search Hyperparameter Tuning:\n",
        "\n",
        "Imagine that your algorithm has 12 different potential hyperparameters and each them can take on 5 different values. Lets say that it takes your laptop 4 seconds to fit each fold of cross validation. For each 5-fold cross-validation it would then take 20 seconds to fit your model and get an accuracy reading reported back. Now imagine that you want to test every possible combination of hyperparameters on your algorithm to get the absolute highest accuracy. You can see how this might become exceedingly tedious and time-consuming to perform by hand. Some hyperparameters (like the C value) have much more than 5 potential values, making hyperparameter tuning a huge task. \n",
        "\n",
        "It is for this reason that more advanced optimization techniques exist, one of which we will be exploring today called GridSearch.\n",
        "\n",
        "### What does GridSearch do?\n",
        "\n",
        "GridSearch takes a dictionary of all of the different hyperparameters that you want to test, and then feeds all of the different combinations through the algorithm for you and then reports back to you which one had the highest accuracy. Pretty slick right? \n",
        "\n",
        "Here is some boilerplate code you can reference to create your implementations:\n",
        "\n",
        "[Chris Albon Logistic Regression sklearn Hyperparameter Tuning with GridSearch](https://chrisalbon.com/machine_learning/model_selection/hyperparameter_tuning_using_grid_search/)"
      ]
    },
    {
      "metadata": {
        "id": "dCZLjm7JjTwh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# These import statements might be useful to you. \n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sb4F7GlnjIvl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create logistic regression object"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nrrfWSJOey0J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a list of all of the different penalty values that you want to test and save them to a variable called 'penalty'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-m1rVmNwfbSH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a list of all of the different C values that you want to test and save them to a variable called 'C'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hy-B_Wm3fz-4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now that you have two lists each holding the different values that you want test, use the dict() function to combine them into a dictionary. \n",
        "# Save your new dictionary to the variable 'hyperparameters'\n",
        "# Print out the dictionary if you're curious as to what it euds up looking like."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v619p6o4j9-3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fit your model using gridsearch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3aUuCQH4gdBc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Print the best penalty and C value from best_model.best_estimator_.get_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IqG4tPaQkKgz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Print out all of the different combinations of your grid search values and their corresponding accuracies.\n",
        "# https://stackoverflow.com/questions/22155953/how-to-print-out-an-accuracy-score-for-each-combination-within-gridsearch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IwOTSHaUkd8J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What hyperparameters give you the highest accuracy? Keep on testing diferent values and report the hyperparameters that give you the highest accuracy."
      ]
    },
    {
      "metadata": {
        "id": "r-ctaux7qOzv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stretch Goals:\n",
        "\n",
        "Explore more advanced automated approaches to hyperparameter tuning. Try and implemenet a random search approach: \n",
        "\n",
        "[Random Search Hyperparameter Tuning](https://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/)\n",
        "\n",
        "Then try a Bayesian Optimization Approach:\n",
        "\n",
        "[Bayesian Optimization](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/)\n",
        "\n",
        "[scikit-optimize](https://scikit-optimize.github.io/notebooks/bayesian-optimization.html)\n",
        "\n",
        "[optunity](http://optunity.readthedocs.io/en/latest/notebooks/notebooks/sklearn-automated-classification.html)\n",
        "\n",
        "You could also try writing a blog post to show how well you understand Cross Validation or Hyperparameter Tuning, both are key concepts to practicing machine learning and would be valuable to demonstrate proficency in.\n"
      ]
    }
  ]
}